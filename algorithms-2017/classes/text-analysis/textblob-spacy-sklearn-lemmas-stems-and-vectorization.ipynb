{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing some packages\n",
    "\n",
    "You'll need to install a handful of packages.\n",
    "\n",
    "```bash\n",
    "pip3 install textblob\n",
    "python3 -m textblob.download_corpora\n",
    "pip3 install spacy\n",
    "python3 -m spacy download en\n",
    "pip3 install scikit-learn\n",
    "```\n",
    "\n",
    "# The libraries we'll be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob: \"Simplified text processing\"\n",
    "\n",
    "Super simple, super easy, super misleading.\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy: \"Industrial-Strength Natural Language Processing\"\n",
    "\n",
    "Super powerful, super incomplete (kind of), super nice website, super full of gotchas.\n",
    "\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn: \"Machine Learning in Python\"\n",
    "\n",
    "Super powerful (in a different way than spaCy), super popular, super not focused on text analysis.\n",
    "\n",
    "http://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit (NLTK): \"A leading platform for building Python programs to work with human language data\"\n",
    "\n",
    "Super old, super popular, super difficult to work with.\n",
    "\n",
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey look we're sentiment analysis experts now\n",
    "\n",
    "With practically no knowledge and absolutely no respect for truth or accuracy, we can know analyze sentiment using TextBlob!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.8, subjectivity=0.9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"I hate driving the car.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.35, subjectivity=0.4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"I don't know if i love driving the car SO MUCH.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.22777777777777777, subjectivity=0.6861111111111112)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = '''I am very disappointed in China. Our foolish past leaders \n",
    "have allowed them to make hundreds of billions of dollars a year in trade, yet\n",
    "they do NOTHING for us with North Korea, just talk. We will no longer allow this\n",
    "to continue. China could easily solve this problem!'''\n",
    "\n",
    "TextBlob(tweet).sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't ever do this unless you are reading movie reviews! You can use [the documentation](https://textblob.readthedocs.io/en/dev/classifiers.html) to build your own easily enough. We're going to talk about how to do it step-by-step in other ways, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words with `.count`\n",
    "\n",
    "When you first start counting words in Python, you'll probably just try out `.count`. Let's take a sentence and see how many times \"fish\" is used in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I tried to fish for fish but I didn't catch any fish\".count(\"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple, right? They don't always need to be sentences, either, you can do this on longer text, too. Paragraphs, multiple lines, even whole books!\n",
    "\n",
    "The big downside is that it's going to count any instance of your word, *even if it's in the middle of a word*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The canny toucan can't recant about the pelican's scant canteloupe\".count(\"can\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That sentence has `\"can\"` in it at most *once*, so we're going to need to do something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "So I guess that isn't going to work! Luckily for us, people worked for billions of years to solve this problem using something called **tokenization**. Tokenization in this situation means \"splitting up a sentence into the parts that matter.\"\n",
    "\n",
    "You can think of it as breaking the sentence apart in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'fish',\n",
       " 'for',\n",
       " 'fish',\n",
       " 'but',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'catch',\n",
       " 'any',\n",
       " 'fish']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_sentence = \"I tried to fish for fish but I didn't catch any fish\"\n",
    "fish_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_sentence = \"I tried to fish for fish but I didn't catch any fish\"\n",
    "fish_list = fish_sentence.split(\" \")\n",
    "fish_list.count(\"fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toucan_sentence = \"The canny toucan can't recant the pelican's scant canteloupe\"\n",
    "toucan_list = toucan_sentence.split(\" \")\n",
    "toucan_list.count(\"can\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our tokenization\n",
    "\n",
    "Splitting on spaces and counting words is fiiiine, but problems start to show up with capital letters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dinner', 'was', 'great', 'tonight,', 'I', 'enjoyed', 'the', 'potatoes.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinner_sentence = \"Dinner was great tonight, I enjoyed the potatoes.\"\n",
    "dinner_list = dinner_sentence.split(\" \")\n",
    "dinner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times does 'dinner' appear?\n",
    "dinner_list.count(\"dinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times does 'potatoes' appear?\n",
    "dinner_list.count(\"potatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinner', 'was', 'great', 'tonight,', 'i', 'enjoyed', 'the', 'potatoes']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinner_sentence = \"Dinner was great tonight, I enjoyed the potatoes.\"\n",
    "dinner_sentence = dinner_sentence.lower().replace(\".\", \"\")\n",
    "dinner_list = dinner_sentence.split(\" \")\n",
    "dinner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinner_list.count(\"dinner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinner_list.count(\"potatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing with TextBlob\n",
    "\n",
    "TextBlob is easy to use, but not as full-featured as spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we'll import TextBlob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "dangerous\n",
      "cats\n",
      "ran\n",
      "dangerously\n",
      "toward\n",
      "dangers\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Then we'll use TextBlob\n",
    "blob = TextBlob(\"The dangerous cats ran dangerously toward dangers.\")\n",
    "\n",
    "for token in blob.tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "dangerous\n",
      "cats\n",
      "ran\n",
      "dangerously\n",
      "toward\n",
      "dangers\n"
     ]
    }
   ],
   "source": [
    "for word in blob.words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['I', 'am', 'a', 'sentence'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"I am a sentence\")\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['у', 'меня', 'зазвонил', 'телефон'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"у меня зазвонил телефон\")\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['私は鉛筆です'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"私は鉛筆です\")\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how it keeps the period when you use `.tokens`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing with spaCy\n",
    "\n",
    "spaCy can do anything, but is a bit more difficult to use than TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we'll import spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, dangerous, cats, ran, dangerously, toward, dangers]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we'll use spaCy\n",
    "doc = nlp(\"The dangerous cats ran dangerously toward dangers\")\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other magic with TextBlob\n",
    "\n",
    "You'll probably wind up using TextBlob for things, so here're some other fun stuff it can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''Today I went driving to the \n",
    "grocery store.  I hate to drive the car, \n",
    "but I love visiting the gas station!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the words\n",
    "\n",
    "This is like tokens, but gets rid of the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Today', 'I', 'went', 'driving', 'to', 'the', 'grocery', 'store', 'I', 'hate', 'to', 'drive', 'the', 'car', 'but', 'I', 'love', 'visiting', 'the', 'gas', 'station'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = TextBlob(text)\n",
    "doc.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['grocery store', 'gas station'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = TextBlob(text)\n",
    "doc.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Today I went driving to the \n",
       " grocery store.\"), Sentence(\"I hate to drive the car, \n",
       " but I love visiting the gas station!\")]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = TextBlob(text)\n",
    "doc.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 13]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(sent.words) for sent in doc.sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'went']),\n",
       " WordList(['went', 'to']),\n",
       " WordList(['to', 'the']),\n",
       " WordList(['the', 'pet']),\n",
       " WordList(['pet', 'store']),\n",
       " WordList(['store', 'to']),\n",
       " WordList(['to', 'buy']),\n",
       " WordList(['buy', 'a']),\n",
       " WordList(['a', 'fish'])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = TextBlob(\"I went to the pet store to buy a fish.\")\n",
    "doc.ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'really']\n",
      "['he', 'screamed']\n"
     ]
    }
   ],
   "source": [
    "doc = TextBlob(\"He really likes cars. I don't like cars, but she said she would buy a car for the moon. He screamed.\".lower())\n",
    "\n",
    "# Shorter way\n",
    "[ngram for ngram in doc.ngrams(2) if ngram[0] == 'he']\n",
    "\n",
    "# Longer way\n",
    "for ngram in doc.ngrams(2):\n",
    "    if ngram[0] == 'he':\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can a word do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'dangerous', 'cats', 'ran', 'dangerously', 'toward', 'dangers'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = TextBlob(\"The dangerous cats ran dangerously toward dangers.\")\n",
    "words = doc.words\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catss'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catssesses'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2].pluralize().pluralize().pluralize().pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stems and lemmas with TextBlob\n",
    "\n",
    "This is something a word can do, but it's *pretty important*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: The | LEMMA: The | STEM: the\n",
      "ORIGINAL: dangerous | LEMMA: dangerous | STEM: danger\n",
      "ORIGINAL: cats | LEMMA: cat | STEM: cat\n",
      "ORIGINAL: ran | LEMMA: ran | STEM: ran\n",
      "ORIGINAL: dangerously | LEMMA: dangerously | STEM: danger\n",
      "ORIGINAL: toward | LEMMA: toward | STEM: toward\n",
      "ORIGINAL: dangers | LEMMA: danger | STEM: danger\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"The dangerous cats ran dangerously toward dangers.\")\n",
    "words = doc.words\n",
    "\n",
    "for word in words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tokens.count('danger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizing in TextBlob **is terrible.** It thinks everything is a noun, and you have to specifically tell it otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairy'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"fairies\").words[0].lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairi'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"fairies\").words[0].stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"cats\").words[0].lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"running\").words[0].lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"running\").words[0].stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"ran\").words[0].stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"running\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"ran\").words[0].lemmatize('v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing with spaCy\n",
    "\n",
    "spaCy is much more powerful than TextBlob, but it's a little more difficult to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we'll import spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The, dangerous, cats, ran, dangerously, toward, dangers, .]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we'll use spaCy\n",
    "doc = nlp(\"The dangerous cats ran dangerously toward dangers.\")\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[501, 2321, 2481, 1022, 18671, 3185, 4541, 453]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we'll use spaCy\n",
    "doc = nlp(\"The dangerous cats ran dangerously toward dangers.\")\n",
    "tokens = [token.lemma for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentiment with spaCy???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment for i love cars is 0.0\n",
      "The sentiment for i hate cars is 0.0\n",
      "The sentiment for i butter cars is 0.0\n",
      "The sentiment for misery and gloomy pain cars is 0.0\n"
     ]
    }
   ],
   "source": [
    "phrases = [\n",
    "    'i love cars', \n",
    "    'i hate cars', \n",
    "    'i butter cars', \n",
    "    'misery and gloomy pain cars'\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "    doc = nlp(phrase)\n",
    "    print(\"The sentiment for\", doc, \"is\", doc[0].sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment for love is Sentiment(polarity=0.5, subjectivity=0.6)\n",
      "The sentiment for hate is Sentiment(polarity=-0.8, subjectivity=0.9)\n",
      "The sentiment for butter is Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "The sentiment for misery and gloomy pain is Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "words = ['love', 'hate', 'butter', 'misery and gloomy pain']\n",
    "for word in words:\n",
    "    blob = TextBlob(word)\n",
    "    print(\"The sentiment for\", word, \"is\", blob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intro to scikit-learn (sklearn)\n",
    "\n",
    "Scikit-learn is for machine learning, which is turns out is *kind of what we're doing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    'i love cars', \n",
    "    'i hate cars', \n",
    "    'cars butter cars', \n",
    "    'misery and gloomy pain cars',\n",
    "    'the cars hate butter'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words into numbers: Vectorization\n",
    "\n",
    "The process of converting words (which computers can't understand) to numbers (which computers *can* understand) is called **vectorization**. So what we're about to do is.... **vectorization!**\n",
    "\n",
    "We take our vectorizer - the `CountVectorizer` - from a part of scikit-learn called `feature_extraction.text`. In machine learning, \"features\" are things that make an object unique. It's how you compare things and classify things and know how each *thing* is different.\n",
    "\n",
    "In this case, our *things* are *sentences*, and you know how they're different because they have different words: our **\"features\"** are **word counts**. That's why this next line reads like it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported the vectorizer - which will convert words to numbers - we need to use the vectorizer to count all of the words in our sentences. `CountVectorizer` always takes a list of documents, never just one! It's kind of boring to compare one document to nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give me a THING that will count words for me!!!!!\n",
    "vec = CountVectorizer()\n",
    "# I have some sentences, please count the words in them\n",
    "matrix = vec.fit_transform(phrases)\n",
    "# What did you find?????\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **sparse matrix** is just a Python way of saying \"we have a list of lists but there isn't much stuff in it so I don't want to show it to you. We'll use **.toarray()** to see the result.\n",
    "\n",
    "Each row is a sentence, and each column is a word. The numbers are how many times that word appears in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our content has been.... VECTORIZED!!!!\n",
    "matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks kind of ugly, though. **Let's make a dataframe out of it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  0  0  1  0  0  1  0  0  0\n",
       "1  0  0  1  0  1  0  0  0  0\n",
       "2  0  1  2  0  0  0  0  0  0\n",
       "3  1  0  1  1  0  0  1  1  0\n",
       "4  0  1  1  0  1  0  0  0  1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little bit nicer, but if only there were a way to get a list of the original words. I don't know which words \"column number 2\" is!\n",
    "\n",
    "**Oh but there is a way to get the words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'butter', 'cars', 'gloomy', 'hate', 'love', 'misery', 'pain', 'the']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And hey, what if we set it in as the column names for the dataframe???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>butter</th>\n",
       "      <th>cars</th>\n",
       "      <th>gloomy</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>misery</th>\n",
       "      <th>pain</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  butter  cars  gloomy  hate  love  misery  pain  the\n",
       "0    0       0     1       0     0     1       0     0    0\n",
       "1    0       0     1       0     1     0       0     0    0\n",
       "2    0       1     2       0     0     0       0     0    0\n",
       "3    1       0     1       1     0     0       1     1    0\n",
       "4    0       1     1       0     1     0       0     0    1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh boy. Such magic. **Which sentence includes 'cars' the most times?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>butter</th>\n",
       "      <th>cars</th>\n",
       "      <th>gloomy</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>misery</th>\n",
       "      <th>pain</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  butter  cars  gloomy  hate  love  misery  pain  the\n",
       "2    0       1     2       0     0     0       0     0    0\n",
       "0    0       0     1       0     0     1       0     0    0\n",
       "1    0       0     1       0     1     0       0     0    0\n",
       "3    1       0     1       1     0     0       1     1    0\n",
       "4    0       1     1       0     1     0       0     0    1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.sort_values(by='cars', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
